{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n",
      "11823\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "path = \"11.deep/d0726/\"\n",
    "path = \"\"\n",
    "chatbotData=pd.read_csv(path+\"ChatBotData.csv\")\n",
    "question, answer = list(chatbotData[\"Q\"]) , list(chatbotData[\"A\"])\n",
    "print(len(question))\n",
    "print(len(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from konlpy.tag import Okt \n",
    "from keras import models, layers, optimizers, metrics, preprocessing \n",
    "\n",
    "# 태그단어 \n",
    "PAD = \"<PADDING>\"  # 패딩 \n",
    "STA = \"<START>\"    # 시작 \n",
    "END = \"<END>\"      # 끝 \n",
    "OOV = \"<OOV>\"      # out of vocabulary \n",
    "PAD_INDEX = 0\n",
    "STA_INDEX = 1 \n",
    "END_INDEX = 2 \n",
    "OOV_INDEX = 3 \n",
    "\n",
    "ENCODER_INPUT = 0\n",
    "DECODER_INPUT = 1\n",
    "DECODER_TARGET = 2\n",
    "\n",
    "# 한 문장에서 단어 시퀀스의 최대 개수 \n",
    "maxSequences = 30 \n",
    "\n",
    "# 임베딩 벡터 차원 \n",
    "embeddingDim = 100 \n",
    "\n",
    "# LSTM 출력 차원 \n",
    "lstmHiddenDim = 128 \n",
    "\n",
    "# 정규표현식 필터 \n",
    "RE_FILTER = re.compile(\"[.,!?\\':;~()]'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23646"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분석기 \n",
    "def posTag(sentences): \n",
    "    tagger = Okt()\n",
    "    sentencePos = []\n",
    "    for sentence in sentences:\n",
    "        # 특수문자 제거 \n",
    "        sentence = re.sub( RE_FILTER  ,  \"\"  , sentence )\n",
    "        sentence = \" \".join(tagger.morphs(sentence))\n",
    "        sentencePos.append(sentence)\n",
    "    return sentencePos \n",
    "    \n",
    "question = posTag(question)\n",
    "answer = posTag(answer)\n",
    "\n",
    "# 질문 + 대답 을 하나로 합치기 \n",
    "sentences = [] \n",
    "sentences.extend(question)\n",
    "sentences.extend(answer)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'바람난'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "# 단어배열 생성 \n",
    "for sentence in sentences : \n",
    "    for word in sentence.split():\n",
    "        words.append(word)\n",
    "\n",
    "\n",
    "# words에서 길이가 0인 단어를 삭제 \n",
    "# 중복단어를 삭제 \n",
    "words = [ word for word in words if len(word)>0 ] # 길이가 0인 단어 삭제 \n",
    "words = list(set(words)) # 중복 단어 삭제 \n",
    "\n",
    "words[:0] = [PAD,STA,END,OOV]\n",
    "\n",
    "# 단어에 대한 인덱스를 부여 -> 딕셔너리\n",
    "wordToIndex = {word:index for index, word in enumerate(words)}\n",
    "indexToWord = {index:word for index, word in enumerate(words)}\n",
    "indexToWord[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 -> 인덱스로 변환\n",
    "def convertTextToIndex(sentences, voc, mytype):\n",
    "    \n",
    "    sentencesIndex=[]\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        sentenceIndex=[]\n",
    "        if mytype == DECODER_INPUT:\n",
    "            \n",
    "            sentenceIndex.extend([voc[STA]])\n",
    "            \n",
    "        for word in sentence.split():\n",
    "            \n",
    "            if voc.get(word) is not None: # 단어에 해당하는 인덱스가 있는 경우\n",
    "                sentenceIndex.extend([voc[word]]) # 단어에 해당되는 인덱스가 추가\n",
    "                \n",
    "            else: # 사전에 없는 단어의 경우 OOV추가\n",
    "                sentenceIndex.extend([voc[OOV]])\n",
    "                \n",
    "        if mytype == DECODER_TARGET:\n",
    "            \n",
    "            # 디코더 출력은 맨 마지막에 end 추가\n",
    "            if maxSequences <= len(sentenceIndex):\n",
    "                sentenceIndex = sentenceIndex[:maxSequences-1] + [voc[END]]\n",
    "                \n",
    "            else:\n",
    "                sentenceIndex += [voc[END]]\n",
    "                \n",
    "        else:\n",
    "            if len(sentenceIndex) > maxSequences:\n",
    "                sentenceIndex = sentenceIndex[:maxSequences]\n",
    "                \n",
    "        # 0으로 채움(pad_sequence)\n",
    "        sentenceIndex += [wordToIndex[PAD]] * (maxSequences-len(sentenceIndex))\n",
    "        \n",
    "        sentencesIndex.append(sentenceIndex)\n",
    "            \n",
    "    return np.asarray(sentencesIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 658 5466 1517    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[    1   777  8433  2604 11914  1193     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "[  777  8433  2604 11914  1193     2     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# 인코더 입력, 디코더 입력, 디코더 출력 -> 인덱스 변환\n",
    "xEncoder = convertTextToIndex(question, wordToIndex, ENCODER_INPUT)\n",
    "print(xEncoder[0])\n",
    "\n",
    "xDecoder = convertTextToIndex(answer, wordToIndex, DECODER_INPUT)\n",
    "print(xDecoder[0])\n",
    "\n",
    "yDecoder = convertTextToIndex(answer, wordToIndex, DECODER_TARGET)\n",
    "print(yDecoder[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.zeros((2,3,4))\n",
    "oneHotData=np.zeros((len(yDecoder),maxSequences,len(words)))\n",
    "#100(답변개수), 30(최대 단어 개수), 454(전체 단어 집합 개수)\n",
    "\n",
    "np.shape(oneHotData)#디코더 출력\n",
    "#하루 가 또 가네요 END\n",
    "for i, seq in enumerate(yDecoder):# (100,30)\n",
    "    for j, index in enumerate(seq):\n",
    "        oneHotData[i,j,index]=1\n",
    "yDecoder=oneHotData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 모델 생성\n",
    "#인코더 정의\n",
    "\n",
    "#입력 문장의 인덱스 sequence를 입력\n",
    "encoderInputs=layers.Input(shape=(None,))\n",
    "#임베딩 계층\n",
    "encoderOutputs=layers.Embedding(len(words),embeddingDim)(encoderInputs)\n",
    "\n",
    "encoderOutputs,stateH, stateC=layers.LSTM(lstmHiddenDim,return_state=True, \n",
    "            dropout=0.2, recurrent_dropout=0.5)(encoderOutputs)\n",
    "#return_state=True => 상태값 리턴\n",
    "#LSTM은 2개 상태 존재(셀, 히든 스테이트)\n",
    "\n",
    "encoderStates=[stateH, stateC]\n",
    "\n",
    "\n",
    "#디코더 정의\n",
    "#출력 문장의 인덱스 sequence를 입력\n",
    "decoderInputs=layers.Input(shape=(None,))\n",
    "#임베딩 계층\n",
    "decoderEmbedding=layers.Embedding(len(words),\n",
    "                                embeddingDim)\n",
    "decoderOutputs=decoderEmbedding(decoderInputs)\n",
    "\n",
    "\n",
    "\n",
    "decoderLSTM=layers.LSTM(lstmHiddenDim,\n",
    "                        return_state=True, \n",
    "            return_sequences=True, \n",
    "                        dropout=0.2, \n",
    "                        recurrent_dropout=0.5)\n",
    "decoderOutputs, _, _=decoderLSTM(decoderOutputs,initial_state=encoderStates)\n",
    "decoderDense=layers.Dense(len(words), \n",
    "                          activation=\"softmax\")\n",
    "decoderOutputs=decoderDense(decoderOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.Model([encoderInputs, decoderInputs],\n",
    "             decoderOutputs)\n",
    "             \n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "             \n",
    "#예측 모델 인코더 정의\n",
    "encoderModel=models.Model(encoderInputs, \n",
    "                          encoderStates)\n",
    "\n",
    "#예측 모델 디코더 정의\n",
    "#바로 앞에 있는 디코더의 출력(상태)을 입력 받아서\n",
    "#예측을 해야 함.\n",
    "decoderStateInputH=layers.Input(shape=(lstmHiddenDim,))\n",
    "decoderStateInputC=layers.Input(shape=(lstmHiddenDim,))\n",
    "decoderStatesInputs=[decoderStateInputH,decoderStateInputC]\n",
    "\n",
    "#임베딩 계층\n",
    "decoderOutputs=decoderEmbedding(decoderInputs)\n",
    "#LSTM 계층\n",
    "decoderOutputs, stateH, stateC=decoderLSTM(decoderOutputs,\n",
    "           initial_state=decoderStatesInputs)\n",
    "decoderStates=[stateH, stateC]\n",
    "\n",
    "#Dense계층을 통해 원핫 형식으로 예측 단어 인덱스를 추출\n",
    "decoderOutputs=decoderDense(decoderOutputs)\n",
    "\n",
    "#예측 모델 디코더 설정\n",
    "decoderModel=models.Model([decoderInputs]+decoderStatesInputs,\n",
    "            [decoderOutputs]+decoderStates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
