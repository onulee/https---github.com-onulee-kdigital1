{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from gensim.models import doc2vec\n",
    "# TaggedDocument는 해당 corpus의 id를 함께 넘겨주는 것을 말함\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. colab : !pip install konlpy 설치해야 함\n",
    "1. https://cholol.tistory.com/466\n",
    "1. https://github.com/tkdlek11112/faq_chatbot_learning/blob/master/DAY1/FAQ_CHATBOT_DAY1.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Did you hear about the Native American man tha...</td>\n",
       "      <td>He nearly drown in his own tea pee.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What's the best anti diarrheal prescription?</td>\n",
       "      <td>Mycheexarphlexin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What do you call a person who is outside a doo...</td>\n",
       "      <td>Matt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           Question  \\\n",
       "0   1  Did you hear about the Native American man tha...   \n",
       "1   2       What's the best anti diarrheal prescription?   \n",
       "2   3  What do you call a person who is outside a doo...   \n",
       "\n",
       "                                Answer  \n",
       "0  He nearly drown in his own tea pee.  \n",
       "1                     Mycheexarphlexin  \n",
       "2                                 Matt  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faqs = pd.read_csv('jokes.csv')\n",
    "faqs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 형태소분석\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Did you hear about the Native American man tha...\n",
       "1         What's the best anti diarrheal prescription?\n",
       "2    What do you call a person who is outside a doo...\n",
       "Name: Question, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faqs['Question'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jmpkorea00\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize 를 하려면, punkt를 다운로드 해야 함\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['did',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'about',\n",
       " 'the',\n",
       " 'native',\n",
       " 'american',\n",
       " 'man',\n",
       " 'that',\n",
       " 'drank',\n",
       " '200',\n",
       " 'cups',\n",
       " 'of',\n",
       " 'tea',\n",
       " '?']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토근화 - 문장을 단어단위로 분리 \n",
    "tokened_questions = [word_tokenize(question.lower()) for question in faqs['Question']]\n",
    "tokened_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos 명사 분리\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jmpkorea00\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize 사용하기 위해 다운로드\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jmpkorea00\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize 사용하기 위해 다운로드\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['did',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'about',\n",
       " 'the',\n",
       " 'native',\n",
       " 'american',\n",
       " 'man',\n",
       " 'that',\n",
       " 'drank',\n",
       " '200',\n",
       " 'cup',\n",
       " 'of',\n",
       " 'tea',\n",
       " '?']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization\n",
    "lemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions]\n",
    "lemmed_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jmpkorea00\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords.words 불용어 제거를 위해 다운로드\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hear', 'native', 'american', 'man', 'drank', '200', 'cup', 'tea', '?']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopword 제거 불용어 제거하기\n",
    "stop_words = stopwords.words('english')\n",
    "questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions]\n",
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트에서 각 문장부분 토큰화\n",
    "\n",
    "index_questions = []\n",
    "for i in range(len(faqs)):\n",
    "    # 불용어 제거한 질문, index와 함께 list로 저장\n",
    "    index_questions.append([questions[i], i ])\n",
    "\n",
    "# Doc2Vec에서 사용하는 TaggedDocument 문서형으로 변경\n",
    "# (영문형태소분석,index) 처리\n",
    "tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['hear', 'native', 'american', 'man', 'drank', '200', 'cup', 'tea', '?'], tags=[0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec 모델(model) 실행\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "d2v_faqs = doc2vec.Doc2Vec(\n",
    "                                vector_size=200,\n",
    "#                                 alpha=0.025,\n",
    "#                                 min_alpha=0.025,\n",
    "                                hs=1,\n",
    "                                negative=0,\n",
    "                                dm=0,\n",
    "                                dbow_words = 1,\n",
    "                                min_count = 5,\n",
    "                                workers = cores,\n",
    "                                seed=0,\n",
    "                                epochs=20\n",
    "                                )\n",
    "d2v_faqs.build_vocab(tagged_questions)\n",
    "d2v_faqs.train(tagged_questions,\n",
    "               total_examples = d2v_faqs.corpus_count,\n",
    "               epochs = d2v_faqs.epochs\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', \"'s\", 'best', 'anti', 'diarrheal', 'prescription', '?']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 테스트 질문, 테스트하는 문장도 같은 전처리를 해준다.\n",
    "test_string = \"What's the best anti diarrheal prescription?\"\n",
    "# 2. 영문 형태소 분석\n",
    "tokened_test_string = word_tokenize(test_string)\n",
    "# 3. pos(명사형)형태 분리\n",
    "lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string]\n",
    "# 4. 불용어 제거\n",
    "test_string = [w for w in lemmed_test_string if not w in stop_words]\n",
    "\n",
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.9330215454101562), (29760, 0.6743109822273254), (30815, 0.6502100825309753), (30143, 0.6352273225784302), (32653, 0.6149254441261292)]\n",
      "1위. 0.9330215454101562, 1 What's the best anti diarrheal prescription?\n",
      "2위. 0.6743109822273254, 29760 What's a Jackhammer's Best Friend?\n",
      "3위. 0.6502100825309753, 30815 What's the best thing about owning a car in Liverpool?\n",
      "4위. 0.6352273225784302, 30143 Why do hummingbirds hum?\n",
      "5위. 0.6149254441261292, 32653 Which U.S. state abbreviation is the best?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmpkorea00\\AppData\\Local\\Temp\\ipykernel_11220\\3885141576.py:3: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  result = d2v_faqs.docvecs.most_similar([test_vector], topn=topn)\n"
     ]
    }
   ],
   "source": [
    "# 벡터화에서 근접한 5개 찾기\n",
    "topn = 5  # 5개설정\n",
    "# 질문 - 벡터화\n",
    "test_vector = d2v_faqs.infer_vector(test_string)\n",
    "# 근접한 5개 찾기\n",
    "result = d2v_faqs.docvecs.most_similar([test_vector], topn=topn)\n",
    "# result[,0] : index, result[:1] : 정확도 \n",
    "print(result)\n",
    "\n",
    "for i in range(topn):\n",
    "    print(\"{}위. {}, {} {}\".format(i+1, result[i][1], result[i][0],faqs['Question'][result[i][0]] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmpkorea00\\AppData\\Local\\Temp\\ipykernel_11220\\226183733.py:10: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  re = d2v_faqs.docvecs.most_similar([tvec], topn = raten)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 = 0.8646162690428284 % (33088/38269 )  \n"
     ]
    }
   ],
   "source": [
    "# 모든 질문을 해서 1위의 결과가 나온 것은 몇개인지 확인, 성능 측정\n",
    "raten = 5\n",
    "found = 0\n",
    "for i in range(len(faqs)):\n",
    "    tstr = faqs['Question'][i]\n",
    "    tokened_test_string = word_tokenize(tstr)\n",
    "    lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string]\n",
    "    ttok = [w for w in lemmed_test_string if not w in stop_words]\n",
    "    tvec = d2v_faqs.infer_vector(ttok)\n",
    "    re = d2v_faqs.docvecs.most_similar([tvec], topn = raten)\n",
    "    for j in range(raten):\n",
    "        if i == re[j][0]: \n",
    "            found = found + 1\n",
    "            break\n",
    "\n",
    "print(\"정확도 = {} % ({}/{} )  \".format(found/len(faqs),found, len(faqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "\n",
    "# 모델 1\n",
    "#d2v_faqs.save(os.path.join('data','/content/drive/My Drive/data/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model'))\n",
    "\n",
    "#모델 2\n",
    "d2v_faqs.save('d2v_faqs_size200_min5_epoch20_jokes.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
